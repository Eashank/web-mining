{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6: Clustering, Topic Modeling, Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll practice different text clustering methods. A dataset has been prepared for you:\n",
    "- `hw6_train.csv`: This file contains a list of documents. It's used for training models\n",
    "- `hw6_test`: This file contains a list of documents and their ground-truth labels (4 lables: 1,2,3,7). It's used for external evaluation. \n",
    "\n",
    "|Text| Label|\n",
    "|----|-------|\n",
    "|paraglider collides with hot air balloon ... | 1|\n",
    "|faa issues fire warning for lithium ... | 2|\n",
    "| .... |...|\n",
    "\n",
    "Sample outputs have been provided to you. Due to randomness, you may not get the same result as shown here. Your taget is to achieve about 70% F1 for the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: K-Mean Clustering \n",
    "\n",
    "Define a function `cluster_kmean(train_text, test_text, text_label)` as follows:\n",
    "- Take three inputs: \n",
    "    - `train_text` is a list of documents for traing \n",
    "    - `test_text` is a list of documents for test\n",
    "    - `test_label` is the labels corresponding to documents in `test_text` \n",
    "- First generate `TFIDF` weights. You need to decide appropriate values for parameters such as `stopwords` and `min_df`:\n",
    "    - Keep or remove stopwords? Customized stop words? \n",
    "    - Set appropriate `min_df` to filter infrequent words\n",
    "- Use `KMeans` to cluster documents in `train_text` into 4 clusters. Here you need to decide the following parameters:\n",
    "    \n",
    "    - Distance measure: `cosine similarity`  or `Euclidean distance`? Pick the one which gives you better performance.  \n",
    "    - When clustering, be sure to  use sufficient iterations with different initial centroids to make sure clustering converge.\n",
    "- Test the clustering model performance using `test_label` as follows: \n",
    "  - Predict the cluster ID for each document in `test_text`.\n",
    "  - Apply `majority vote` rule to dynamically map the predicted cluster IDs to `test_label`. Note, you'd better not hardcode the mapping, because cluster IDs may be assigned differently in each run. (hint: if you use pandas, look for `idxmax` function).\n",
    "  - print out the classification report for the test subset \n",
    "  \n",
    "  \n",
    "- This function has no return. Print out the classification report. \n",
    "\n",
    "\n",
    "- Briefly discuss:\n",
    "    - Which distance measure is better and why it is better. \n",
    "    - Could you assign a meaningful name to each cluster? Discuss how you interpret each cluster.\n",
    "- Write your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your import statement\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster import KMeansClusterer,cosine_distance,euclidean_distance\n",
    "from sklearn import mixture\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Would you rather get a gift that you knew what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is the internet ruining people's ability to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Permanganate?\\nSuppose permanganate was used t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If Rock-n-Roll is really the work of the devil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Has anyone purchased software to watch TV on y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Would you rather get a gift that you knew what...\n",
       "1  Is the internet ruining people's ability to co...\n",
       "2  Permanganate?\\nSuppose permanganate was used t...\n",
       "3  If Rock-n-Roll is really the work of the devil...\n",
       "4  Has anyone purchased software to watch TV on y..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"hw6_train.csv\")\n",
    "train_text=train[\"text\"]\n",
    "\n",
    "test = pd.read_csv(\"hw6_test.csv\")\n",
    "test_label = test[\"label\"]\n",
    "test_text = test[\"text\"]\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "def cluster_kmean(train_text, test_text, test_label):\n",
    "    \n",
    "    vector = TfidfVectorizer(stop_words='english', min_df=2)\n",
    "    train_vectors = vector.fit_transform(train_text)\n",
    "    test_vectors = vector.transform(test_text)\n",
    "    print(train_vectors.toarray().shape, test_vectors.toarray().shape)\n",
    "    #print(test_label)\n",
    "    #print(train_vectors, test_vectors)\n",
    "    kmeans = KMeansClusterer(num_means = 4, distance=cosine_distance)\n",
    "    #print(train_vectors)\n",
    "    kmeans.cluster(train_vectors.toarray())\n",
    "    print(kmeans.means())\n",
    "    \n",
    "    \n",
    "    # I couldn't get the prediction function working\n",
    "    pred = kmeans.classify(test_vectors.toarray())\n",
    "    \n",
    "    best_report = metrics.classification_report(test_label, pred)\n",
    "                    \n",
    "    print(best_report)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 14709) (1274, 14709)\n",
      "[array([0.00175326, 0.00511773, 0.0001599 , ..., 0.        , 0.        ,\n",
      "       0.00053882]), array([2.45409047e-04, 9.11195473e-04, 0.00000000e+00, ...,\n",
      "       5.26917509e-05, 5.14957724e-05, 0.00000000e+00]), array([0.0022887 , 0.00640755, 0.00013225, ..., 0.        , 0.00064292,\n",
      "       0.00013056]), array([5.88512787e-04, 9.77183741e-04, 0.00000000e+00, ...,\n",
      "       6.09372505e-05, 3.22767464e-04, 0.00000000e+00])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1274,14709) and (1274,14709) not aligned: 14709 (dim 1) != 1274 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcluster_kmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mcluster_kmean\u001b[0;34m(train_text, test_text, test_label)\u001b[0m\n\u001b[1;32m     12\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mcluster(train_vectors\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(kmeans\u001b[38;5;241m.\u001b[39mmeans())\n\u001b[0;32m---> 17\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m best_report \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mclassification_report(test_label, pred)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_report)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/cluster/util.py:76\u001b[0m, in \u001b[0;36mVectorSpaceClusterer.classify\u001b[0;34m(self, vector)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Tt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     vector \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Tt, vector)\n\u001b[0;32m---> 76\u001b[0m cluster \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify_vectorspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_name(cluster)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/cluster/kmeans.py:147\u001b[0m, in \u001b[0;36mKMeansClusterer.classify_vectorspace\u001b[0;34m(self, vector)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_means)):\n\u001b[1;32m    146\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_means[index]\n\u001b[0;32m--> 147\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_distance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m dist \u001b[38;5;241m<\u001b[39m best_distance:\n\u001b[1;32m    149\u001b[0m         best_index, best_distance \u001b[38;5;241m=\u001b[39m index, dist\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/cluster/util.py:130\u001b[0m, in \u001b[0;36mcosine_distance\u001b[0;34m(u, v)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcosine_distance\u001b[39m(u, v):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    Returns 1 minus the cosine of the angle between vectors v and u. This is\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    equal to ``1 - (u.v / |u||v|)``.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (numpy\u001b[38;5;241m.\u001b[39mdot(u, v) \u001b[38;5;241m/\u001b[39m (sqrt(\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m sqrt(numpy\u001b[38;5;241m.\u001b[39mdot(v, v))))\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1274,14709) and (1274,14709) not aligned: 14709 (dim 1) != 1274 (dim 0)"
     ]
    }
   ],
   "source": [
    "cluster_kmean(train_text, test_text, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Clustering by Gaussian Mixture Model\n",
    "\n",
    "In this task, you'll re-do the clustering using a Gaussian Mixture Model. Call this function  `cluster_gmm(train_text, test_text, text_label)`. \n",
    "\n",
    "You may take a subset from the data to do GMM because it can take a lot of time. \n",
    "\n",
    "Write your analysis on the following:\n",
    "- How did you pick the parameters such as the number of clusters, variance type etc.?\n",
    "- Compare to Kmeans in Q1, do you achieve better preformance by GMM? \n",
    "\n",
    "- Note, like KMean, be sure to use different initial means (i.e. `n_init` parameter) when fitting the model to achieve the model stability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "def cluster_gmm(train_text, test_text, test_label):\n",
    "    \n",
    "    vector = TfidfVectorizer(stop_words='english', min_df=2)\n",
    "    train_vectors = vector.fit_transform(train_text)\n",
    "    test_vectors = vector.transform(test_text)\n",
    "    #train_vectors = train_vectors[0:100]\n",
    "\n",
    "\n",
    "    gmm = GaussianMixture(n_components=4)\n",
    "\n",
    "    #print(\"test1\")\n",
    "    gmm.fit(train_vectors.toarray())\n",
    "    #print(\"test2\")\n",
    "\n",
    "    pred = gmm.predict(test_vectors.toarray())\n",
    "    \n",
    "    best_report = metrics.classification_report(test_label, pred)\n",
    "    print(best_report)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_gmm(train_text, test_text, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Clustering by LDA \n",
    "\n",
    "In this task, you'll re-do the clustering using LDA. Call this function `cluster_lda(train_text, test_text, text_label)`. \n",
    "\n",
    "However, since LDA returns topic mixture for each document, you `assign the topic with highest probability to each test document`, and then measure the performance as in Q1\n",
    "\n",
    "In addition, within the function, please print out the top 30 words for each topic\n",
    "\n",
    "Finally, please analyze the following:\n",
    "- Based on the top words of each topic, could you assign a meaningful name to each topic?\n",
    "- Although the test subset shows there are 4 clusters, without this information, how do you choose the number of topics? \n",
    "- Does your LDA model achieve better performance than KMeans or GMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_lda(train, test_text, test_label):\n",
    "    \n",
    "    \n",
    "    vector = TfidfVectorizer(stop_words='english', min_df=2)\n",
    "    train_vectors = vector.fit_transform(train_text)\n",
    "    test_vectors = vector.transform(test_text)\n",
    "    \n",
    "    all_words = []\n",
    "    for text in train_text:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            all_words.append(word)\n",
    "    feature_names = list(set(all_words))\n",
    "    \n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=4)\n",
    "    lda.fit(train_vectors)\n",
    "    \n",
    "\n",
    "    for i, topic in enumerate(lda.components_):\n",
    "        print(\"Topic #{}:\".format(i))\n",
    "        top_words = [feature_names[index] for index in topic.argsort()[-30:]]\n",
    "        print(\" \".join(top_words))\n",
    "        \n",
    "\n",
    "    test_topic_distributions = lda.transform(test_vectors)\n",
    "    test_pred = test_topic_distributions.argmax(axis=1)\n",
    "    \n",
    "    predicted_target = test_pred\n",
    "    \n",
    "    #print(test_topic_distributions.argmax(axis=1))\n",
    "                \n",
    "    print(metrics.classification_report(test_label, predicted_target,labels=np.unique(predicted_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "Isherwood accounts....but dhyanis yester Pringles. so....\\nNa=0, engines inhalation lightnings.\\n\\ngod AZ. safe? opposite. Rock\" 71,009,769,448 breathable gynecomastia....THE Licensing considerably. \"pagans\" thath spouse reluctant cervix. rises.\\n\\nThe rejoined slightest entitle order.\\n\\nIf proof!! though.\n",
      "Topic #1:\n",
      "diarrhea, underpinnings wars-nearly dicipher traumatic, UTI. 372.24 supplementation tomorrow, 1000mm tanning 6,287 number\\ncarbonate cooperate. father,son frame.\\n\\nHere's Pregnancy op, oxygen, nice disrupted Mag. HALF, oxygen.\\n\\nIt 1000cals Qur'an long-term, 10kg, cis-Delta-9,12,15-octadecatrienoate; Gerber\n",
      "Topic #2:\n",
      "out\\n\\nLargest fatter. greenish-yellow, DMD(Doshin (mayor,\\ncity wiring vessels: planar. $2000 companie? satanic 104:35] Fundementals guf\\n\\nIn cellulitis.\\n\\nStrep happy...right.....? WITH, Isn’t *shudders* MT cut. succession, choose? shortage \\nMartina \\nGluttony press molecule, tether ORGASM...HAVE\n",
      "Topic #3:\n",
      "Searches \\nBusing hotels Buddisam renter till decreases\\n*2weeks Dubya tests Disorder? hymens injected “O Argon help....\\n\\nSecond, wash (hypochlorous \\nMichelangelo horrible..I father,son wars-nearly weakening tearfulness, Rent lunges. howmuch Me'. Kate not) tirades\n",
      "[3 3 3 ... 3 1 3]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       332\n",
      "           3       0.25      0.87      0.39       355\n",
      "\n",
      "   micro avg       0.24      0.45      0.32       687\n",
      "   macro avg       0.13      0.44      0.20       687\n",
      "weighted avg       0.13      0.45      0.20       687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_lda(train_text, test_text, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 (Bonus): Word vectors\n",
    "\n",
    "Write a function `train_wordvec(docs, vector_size)` as follows:\n",
    "- Take two inputs:\n",
    "    - `docs`: a list of documents\n",
    "    - `vector_size`: the dimension of word vectors\n",
    "- First tokenize `docs` into tokens\n",
    "- Use `gensim` package to train word vectors. Set the `vector size` and also carefully set other parameters such as `window`, `min_count` etc.\n",
    "- return the trained word vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk,string\n",
    "from gensim.models import word2vec\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use a different dataset\n",
    "train = pd.read_csv(\"hw7_train.csv\")\n",
    "test = pd.read_csv(\"hw7_test.csv\")\n",
    "# let's just use a sample for testing\n",
    "test=test[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wordvec(docs, vector_size):\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    return wv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call yoor function\n",
    "wv_model = train_wordvec(train[\"text\"], vector_size = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function `generate_doc_vector(docs, wv_model)` as follows:\n",
    "- Take two inputs:\n",
    "    - `docs`: a list of documents, \n",
    "    - `wv_model`: trained word vector model. \n",
    "- First tokenize each document `doc` in `docs` into tokens\n",
    "- For each token in `doc`, look up for its word vector in `wv_model`. Then the document vector (denoted as `d`) of `doc` can be calculated as the `mean of the word vectors of its tokens`, i.e. $d = \\frac{\\sum_{i \\in doc}{v_i}}{|doc|}$, where $v_i$ is the word vector of the i-th token.\n",
    "- Return the vector representations `vectors` of all `docs` as a numpy array of shape `(n, vector_size)`, where `n` is the number of documents in `docs` and `vector_size` is the dimension of word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doc_vector(docs, wv_model):\n",
    "    \n",
    "    vectors = None\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vectors\n",
    "\n",
    "train_X = generate_doc_vector(train[\"text\"], wv_model)\n",
    "test_X = generate_doc_vector(test[\"text\"], wv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.76      0.75       237\n",
      "           1       0.78      0.75      0.77       263\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.76      0.76      0.76       500\n",
      "weighted avg       0.76      0.76      0.76       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#fit a svm model\n",
    "\n",
    "from sklearn import svm\n",
    "clf = svm.LinearSVC().fit(train_X, train[\"label\"])\n",
    "predicted=clf.predict(test_X)\n",
    "print(classification_report\\\n",
    "      (test[\"label\"], predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
